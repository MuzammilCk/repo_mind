## ðŸŽ¯ Project Overview

Transform GitHub repositories into actionable reports using:
- **FastAPI**: RESTful API layer for all tools
- **Gemini Interaction API**: Orchestration brain with memory
- **Vector DB (FAISS)**: Persistent memory layer
- **Multi-tool Integration**: repo2txt, SeaGOAT, CodeQL

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Ingest   â”‚  â”‚ SeaGOAT  â”‚  â”‚ CodeQL   â”‚  â”‚ Analyze â”‚ â”‚
â”‚  â”‚ API      â”‚  â”‚ API      â”‚  â”‚ API      â”‚  â”‚ API     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Gemini Interaction API (Orchestrator)         â”‚
â”‚  â€¢ Planning & Task Decomposition                         â”‚
â”‚  â€¢ Stateful Conversations (previous_interaction_id)      â”‚
â”‚  â€¢ Function Calling to Tool APIs                         â”‚
â”‚  â€¢ Thinking & Reasoning                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Memory Layer (FAISS)                   â”‚
â”‚  â€¢ Past interactions                                     â”‚
â”‚  â€¢ Tool results cache                                    â”‚
â”‚  â€¢ Contextual embeddings                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“‹ Prerequisites

```bash
# Required installations
pip install fastapi[standard] uvicorn
pip install google-genai>=1.55.0
pip install faiss-cpu sentence-transformers
pip install gitpython requests pydantic

# External tools
# - CodeQL CLI: https://github.com/github/codeql-cli-binaries
# - SeaGOAT: pip install seagoat
# - repo2txt: pip install repo2txt (or custom implementation)
```

## ðŸ—‚ï¸ Project Structure

```
repo-analyzer/
â”œâ”€â”€ main.py                      # FastAPI application entry
â”œâ”€â”€ config.py                    # Configuration management
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ api/                         # FastAPI routers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest.py               # Ingest endpoints
â”‚   â”œâ”€â”€ search.py               # SeaGOAT endpoints
â”‚   â”œâ”€â”€ analysis.py             # CodeQL endpoints
â”‚   â””â”€â”€ orchestrator.py         # Main orchestration endpoint
â”‚
â”œâ”€â”€ services/                    # Business logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest_service.py       # repo2txt integration
â”‚   â”œâ”€â”€ search_service.py       # SeaGOAT integration
â”‚   â”œâ”€â”€ codeql_service.py       # CodeQL integration
â”‚   â””â”€â”€ gemini_service.py       # Gemini Interaction API
â”‚
â”œâ”€â”€ models/                      # Pydantic models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ requests.py             # Request models
â”‚   â””â”€â”€ responses.py            # Response models
â”‚
â”œâ”€â”€ memory/                      # Memory layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vector_store.py         # FAISS operations
â”‚   â””â”€â”€ interaction_store.py    # Interaction history
â”‚
â””â”€â”€ utils/                       # Utilities
    â”œâ”€â”€ __init__.py
    â””â”€â”€ helpers.py              # Helper functions
```

---
## Current Progress

**Phase 1: FastAPI Foundation & Tool APIs** - 90% Complete (9/10 modules)

### Completed Modules âœ…

1. **Module 1.1** - Configuration & Environment Management
2. **Module 1.2** - Pydantic Models (Requests & Responses)
3. **Module 1.3** - Ingest Service (Repository cloning & processing)
4. **Module 1.4** - SeaGOAT Service (Semantic code search)
5. **Module 1.5** - CodeQL Service (Static analysis)
6. **Module 1.6** - API Routers (Ingest, Search, Analysis endpoints)
7. **Module 1.7** - Orchestrator Skeleton (Plan creation & execution with HMAC)
8. **Module 1.8** - Logging & Telemetry (Structured logging, metrics, DEBUG mode)
9. **Module 1.9** - Tests & CI (41+ test cases, pytest, GitHub Actions)

### Next Module

10. **Module 1.10** - Anti-Hallucination & Safety (Evidence validation, source citations)

---

# Phase 1: FastAPI Foundation & Tool APIs

## Step 1.1: Configuration Setup âœ… COMPLETED (2026-01-31)

**File: `config.py`**

```python
from pydantic_settings import BaseSettings
from typing import Optional
import os

class Settings(BaseSettings):
    # API Configuration
    API_TITLE: str = "Repo Analyzer API"
    API_VERSION: str = "1.0.0"
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    
    # Gemini Configuration
    GEMINI_API_KEY: str
    GEMINI_MODEL: str = "gemini-2.5-flash"
    
    # Storage Paths
    WORKSPACE_DIR: str = "./workspace"
    INGEST_DIR: str = "./workspace/ingest"
    OUTPUT_DIR: str = "./workspace/output"
    MEMORY_DIR: str = "./workspace/memory"
    
    # CodeQL Configuration
    CODEQL_PATH: str = "codeql"  # Path to CodeQL CLI
    CODEQL_DB_DIR: str = "./workspace/codeql-dbs"
    
    # SeaGOAT Configuration
    SEAGOAT_PORT: int = 8765
    
    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()

# Ensure directories exist
os.makedirs(settings.WORKSPACE_DIR, exist_ok=True)
os.makedirs(settings.INGEST_DIR, exist_ok=True)
os.makedirs(settings.OUTPUT_DIR, exist_ok=True)
os.makedirs(settings.MEMORY_DIR, exist_ok=True)
os.makedirs(settings.CODEQL_DB_DIR, exist_ok=True)
```

**File: `.env.example`**

```env
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.5-flash
CODEQL_PATH=/path/to/codeql
WORKSPACE_DIR=./workspace
```

## Step 1.2: Pydantic Models âœ… COMPLETED (2026-01-31)

**File: `models/requests.py`**

```python
from pydantic import BaseModel, Field, HttpUrl
from typing import Optional, List, Dict, Any
from enum import Enum

class RepoSource(BaseModel):
    """Repository source"""
    url: Optional[HttpUrl] = None
    local_path: Optional[str] = None
    
    class Config:
        json_schema_extra = {
            "example": {
                "url": "https://github.com/user/repo"
            }
        }

class IngestRequest(BaseModel):
    """Request to ingest a repository"""
    source: RepoSource
    include_patterns: Optional[List[str]] = Field(default=["*.py", "*.js", "*.java"])
    exclude_patterns: Optional[List[str]] = Field(default=["node_modules/*", ".git/*"])
    
class SemanticSearchRequest(BaseModel):
    """Request for semantic code search"""
    repo_id: str
    query: str
    limit: int = Field(default=10, ge=1, le=50)

class CodeQLScanRequest(BaseModel):
    """Request to run CodeQL analysis"""
    repo_id: str
    language: str = Field(default="python", description="Primary language")
    query_suite: Optional[str] = Field(default="security-extended", 
                                       description="CodeQL query suite")

class AnalysisRequest(BaseModel):
    """Main analysis request - orchestrates everything"""
    source: RepoSource
    analysis_type: str = Field(default="full", 
                               description="full, security, architecture, or custom")
    custom_instructions: Optional[str] = None
    include_semantic_search: bool = True
    include_security_scan: bool = True
    previous_interaction_id: Optional[str] = None  # For memory/context
```

**File: `models/responses.py`**

```python
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from datetime import datetime

class IngestResponse(BaseModel):
    """Response from ingest operation"""
    repo_id: str
    status: str
    file_count: int
    total_lines: int
    languages: Dict[str, int]
    repo_md_path: str
    tree_json_path: str
    created_at: datetime

class SearchResult(BaseModel):
    """Single semantic search result"""
    file_path: str
    line_number: int
    code_snippet: str
    relevance_score: float
    context: str

class SemanticSearchResponse(BaseModel):
    """Response from semantic search"""
    repo_id: str
    query: str
    results: List[SearchResult]
    total_results: int

class CodeQLFinding(BaseModel):
    """Single CodeQL finding"""
    rule_id: str
    severity: str
    message: str
    file_path: str
    start_line: int
    end_line: int
    recommendation: Optional[str] = None

class CodeQLResponse(BaseModel):
    """Response from CodeQL scan"""
    repo_id: str
    language: str
    findings: List[CodeQLFinding]
    total_findings: int
    critical_count: int
    high_count: int
    medium_count: int
    low_count: int

class Issue(BaseModel):
    """Analyzed issue with fix recommendations"""
    title: str
    description: str
    severity: str
    evidence: List[str]  # File paths, line numbers, or CodeQL findings
    fix_steps: List[str]
    priority: int

class AnalysisResponse(BaseModel):
    """Complete analysis response"""
    repo_id: str
    interaction_id: str  # Gemini interaction ID for follow-ups
    architecture_summary: str
    top_issues: List[Issue]
    recommendations: List[str]
    report_path: str
    raw_report_json: Dict[str, Any]
    created_at: datetime
```

## Step 1.3: Ingest Service (repo2txt wrapper) âœ… COMPLETED (2026-01-31)

**File: `services/ingest_service.py`**

```python
import os
import subprocess
import json
import uuid
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import git

from config import settings
from models.requests import IngestRequest
from models.responses import IngestResponse

class IngestService:
    """Service for ingesting repositories"""
    
    def __init__(self):
        self.ingest_dir = Path(settings.INGEST_DIR)
        self.ingest_dir.mkdir(parents=True, exist_ok=True)
    
    def ingest_repository(self, request: IngestRequest) -> IngestResponse:
        """
        Ingest a repository using repo2txt or custom implementation
        """
        repo_id = str(uuid.uuid4())[:8]
        repo_dir = self.ingest_dir / repo_id
        repo_dir.mkdir(parents=True, exist_ok=True)
        
        # Step 1: Clone or copy repository
        if request.source.url:
            local_repo_path = self._clone_repository(str(request.source.url), repo_dir / "source")
        elif request.source.local_path:
            local_repo_path = Path(request.source.local_path)
        else:
            raise ValueError("Either url or local_path must be provided")
        
        # Step 2: Generate repo.md using repo2txt
        repo_md_path, stats = self._generate_repo_md(
            local_repo_path,
            repo_dir / "repo.md",
            request.include_patterns,
            request.exclude_patterns
        )
        
        # Step 3: Generate tree.json
        tree_json_path = self._generate_tree_json(local_repo_path, repo_dir / "tree.json")
        
        return IngestResponse(
            repo_id=repo_id,
            status="completed",
            file_count=stats["file_count"],
            total_lines=stats["total_lines"],
            languages=stats["languages"],
            repo_md_path=str(repo_md_path),
            tree_json_path=str(tree_json_path),
            created_at=datetime.utcnow()
        )
    
    def _clone_repository(self, url: str, target_dir: Path) -> Path:
        """Clone a git repository"""
        try:
            git.Repo.clone_from(url, target_dir, depth=1)
            return target_dir
        except Exception as e:
            raise RuntimeError(f"Failed to clone repository: {str(e)}")
    
    def _generate_repo_md(
        self,
        repo_path: Path,
        output_path: Path,
        include_patterns: List[str],
        exclude_patterns: List[str]
    ) -> tuple[Path, Dict]:
        """
        Generate repo.md using repo2txt or custom implementation
        """
        try:
            # Try using repo2txt first
            result = subprocess.run(
                ["repo2txt", str(repo_path), "-o", str(output_path)],
                capture_output=True,
                text=True,
                timeout=300
            )
            
            if result.returncode == 0:
                stats = self._calculate_stats(output_path, repo_path)
                return output_path, stats
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        
        # Fallback: Custom implementation
        return self._custom_repo_to_md(repo_path, output_path, include_patterns, exclude_patterns)
    
    def _custom_repo_to_md(
        self,
        repo_path: Path,
        output_path: Path,
        include_patterns: List[str],
        exclude_patterns: List[str]
    ) -> tuple[Path, Dict]:
        """
        Custom implementation to convert repo to markdown
        """
        from fnmatch import fnmatch
        
        stats = {"file_count": 0, "total_lines": 0, "languages": {}}
        
        with open(output_path, 'w', encoding='utf-8') as md_file:
            md_file.write(f"# Repository: {repo_path.name}\n\n")
            md_file.write(f"Generated: {datetime.utcnow().isoformat()}\n\n")
            md_file.write("---\n\n")
            
            for file_path in repo_path.rglob('*'):
                if not file_path.is_file():
                    continue
                
                # Check exclusions
                rel_path = file_path.relative_to(repo_path)
                if any(fnmatch(str(rel_path), pattern) for pattern in exclude_patterns):
                    continue
                
                # Check inclusions
                if include_patterns and not any(fnmatch(file_path.name, pattern) for pattern in include_patterns):
                    continue
                
                # Get file extension for language stats
                ext = file_path.suffix
                stats["languages"][ext] = stats["languages"].get(ext, 0) + 1
                stats["file_count"] += 1
                
                # Write file content
                try:
                    content = file_path.read_text(encoding='utf-8')
                    lines = content.count('\n')
                    stats["total_lines"] += lines
                    
                    md_file.write(f"## File: {rel_path}\n\n")
                    md_file.write(f"```{ext[1:] if ext else 'text'}\n")
                    md_file.write(content)
                    md_file.write("\n```\n\n")
                except Exception as e:
                    md_file.write(f"*Error reading file: {str(e)}*\n\n")
        
        return output_path, stats
    
    def _generate_tree_json(self, repo_path: Path, output_path: Path) -> Path:
        """Generate tree.json with repository structure"""
        
        def build_tree(path: Path) -> Dict:
            if path.is_file():
                return {
                    "type": "file",
                    "name": path.name,
                    "size": path.stat().st_size
                }
            
            children = []
            try:
                for child in sorted(path.iterdir()):
                    if child.name.startswith('.'):
                        continue
                    children.append(build_tree(child))
            except PermissionError:
                pass
            
            return {
                "type": "directory",
                "name": path.name,
                "children": children
            }
        
        tree = build_tree(repo_path)
        
        with open(output_path, 'w') as f:
            json.dump(tree, f, indent=2)
        
        return output_path
    
    def _calculate_stats(self, repo_md_path: Path, repo_path: Path) -> Dict:
        """Calculate repository statistics"""
        stats = {"file_count": 0, "total_lines": 0, "languages": {}}
        
        for file_path in repo_path.rglob('*'):
            if file_path.is_file():
                stats["file_count"] += 1
                ext = file_path.suffix
                stats["languages"][ext] = stats["languages"].get(ext, 0) + 1
                
                try:
                    lines = file_path.read_text(encoding='utf-8').count('\n')
                    stats["total_lines"] += lines
                except:
                    pass
        
        return stats
    
    def get_repo_content(self, repo_id: str) -> str:
        """Get the repo.md content for a repository"""
        repo_md_path = self.ingest_dir / repo_id / "repo.md"
        if not repo_md_path.exists():
            raise FileNotFoundError(f"Repository {repo_id} not found")
        
        return repo_md_path.read_text(encoding='utf-8')
```

## Step 1.4: SeaGOAT Service âœ… COMPLETED (2026-01-31)

**File: `services/search_service.py`**

```python
import subprocess
import json
import asyncio
from pathlib import Path
from typing import List
import os

from config import settings
from models.requests import SemanticSearchRequest
from models.responses import SemanticSearchResponse, SearchResult

class SearchService:
    """Service for semantic code search using SeaGOAT"""
    
    def __init__(self):
        self.ingest_dir = Path(settings.INGEST_DIR)
        self.seagoat_servers = {}  # repo_id -> process
    
    async def index_repository(self, repo_id: str) -> bool:
        """
        Start SeaGOAT server for a repository and index it
        """
        repo_source_dir = self.ingest_dir / repo_id / "source"
        
        if not repo_source_dir.exists():
            raise FileNotFoundError(f"Repository source not found: {repo_id}")
        
        # Start SeaGOAT server in background
        try:
            # Initialize SeaGOAT for the repository
            result = subprocess.run(
                ["seagoat-server", "start", str(repo_source_dir)],
                cwd=str(repo_source_dir),
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode == 0:
                # Wait for indexing to complete
                await asyncio.sleep(5)
                return True
            else:
                raise RuntimeError(f"SeaGOAT indexing failed: {result.stderr}")
                
        except Exception as e:
            raise RuntimeError(f"Failed to index repository: {str(e)}")
    
    async def search(self, request: SemanticSearchRequest) -> SemanticSearchResponse:
        """
        Perform semantic search on indexed repository
        """
        repo_source_dir = self.ingest_dir / request.repo_id / "source"
        
        if not repo_source_dir.exists():
            raise FileNotFoundError(f"Repository not found: {request.repo_id}")
        
        try:
            # Use SeaGOAT CLI for search
            result = subprocess.run(
                ["seagoat", request.query, str(repo_source_dir)],
                capture_output=True,
                text=True,
                timeout=30,
                cwd=str(repo_source_dir)
            )
            
            if result.returncode != 0:
                raise RuntimeError(f"SeaGOAT search failed: {result.stderr}")
            
            # Parse SeaGOAT output
            search_results = self._parse_seagoat_output(result.stdout, request.limit)
            
            return SemanticSearchResponse(
                repo_id=request.repo_id,
                query=request.query,
                results=search_results,
                total_results=len(search_results)
            )
            
        except subprocess.TimeoutExpired:
            raise RuntimeError("Search timeout exceeded")
        except Exception as e:
            raise RuntimeError(f"Search failed: {str(e)}")
    
    def _parse_seagoat_output(self, output: str, limit: int) -> List[SearchResult]:
        """
        Parse SeaGOAT CLI output into SearchResult objects
        """
        results = []
        
        # SeaGOAT output format parsing (adjust based on actual output)
        lines = output.strip().split('\n')
        
        current_file = None
        current_line = None
        current_snippet = []
        current_score = 0.0
        
        for line in lines:
            if line.startswith('File:'):
                if current_file and current_snippet:
                    results.append(SearchResult(
                        file_path=current_file,
                        line_number=current_line or 0,
                        code_snippet='\n'.join(current_snippet),
                        relevance_score=current_score,
                        context=""
                    ))
                    current_snippet = []
                
                current_file = line.replace('File:', '').strip()
                
            elif line.startswith('Line:'):
                current_line = int(line.replace('Line:', '').strip())
                
            elif line.startswith('Score:'):
                current_score = float(line.replace('Score:', '').strip())
                
            elif current_file:
                current_snippet.append(line)
            
            if len(results) >= limit:
                break
        
        # Add last result
        if current_file and current_snippet and len(results) < limit:
            results.append(SearchResult(
                file_path=current_file,
                line_number=current_line or 0,
                code_snippet='\n'.join(current_snippet),
                relevance_score=current_score,
                context=""
            ))
        
        return results[:limit]
```

---

# Phase 2: CodeQL Integration

## Step 2.1: CodeQL Service

**File: `services/codeql_service.py`**

```python
import subprocess
import json
import shutil
from pathlib import Path
from typing import Dict, List
import os

from config import settings
from models.requests import CodeQLScanRequest
from models.responses import CodeQLResponse, CodeQLFinding

class CodeQLService:
    """Service for CodeQL static analysis"""
    
    def __init__(self):
        self.codeql_path = settings.CODEQL_PATH
        self.db_dir = Path(settings.CODEQL_DB_DIR)
        self.ingest_dir = Path(settings.INGEST_DIR)
        self.db_dir.mkdir(parents=True, exist_ok=True)
        
        # Verify CodeQL installation
        self._verify_codeql()
    
    def _verify_codeql(self):
        """Verify CodeQL CLI is installed and accessible"""
        try:
            result = subprocess.run(
                [self.codeql_path, "version"],
                capture_output=True,
                text=True,
                timeout=10
            )
            if result.returncode != 0:
                raise RuntimeError("CodeQL CLI not working properly")
        except Exception as e:
            raise RuntimeError(f"CodeQL not found or not working: {str(e)}")
    
    def analyze_repository(self, request: CodeQLScanRequest) -> CodeQLResponse:
        """
        Run CodeQL analysis on a repository
        """
        repo_source_dir = self.ingest_dir / request.repo_id / "source"
        
        if not repo_source_dir.exists():
            raise FileNotFoundError(f"Repository source not found: {request.repo_id}")
        
        db_name = f"{request.repo_id}-db"
        db_path = self.db_dir / db_name
        
        # Step 1: Create CodeQL database
        self._create_database(repo_source_dir, db_path, request.language)
        
        # Step 2: Run queries
        results_path = self.db_dir / f"{request.repo_id}-results.sarif"
        self._run_queries(db_path, request.language, request.query_suite, results_path)
        
        # Step 3: Parse results
        findings = self._parse_sarif(results_path)
        
        # Count severity levels
        severity_counts = self._count_severities(findings)
        
        return CodeQLResponse(
            repo_id=request.repo_id,
            language=request.language,
            findings=findings,
            total_findings=len(findings),
            critical_count=severity_counts.get("critical", 0),
            high_count=severity_counts.get("high", 0),
            medium_count=severity_counts.get("medium", 0),
            low_count=severity_counts.get("low", 0)
        )
    
    def _create_database(self, source_dir: Path, db_path: Path, language: str):
        """Create CodeQL database from source code"""
        
        # Remove existing database if present
        if db_path.exists():
            shutil.rmtree(db_path)
        
        try:
            result = subprocess.run(
                [
                    self.codeql_path, "database", "create",
                    str(db_path),
                    f"--language={language}",
                    f"--source-root={source_dir}",
                    "--overwrite"
                ],
                capture_output=True,
                text=True,
                timeout=600  # 10 minutes timeout
            )
            
            if result.returncode != 0:
                raise RuntimeError(f"Database creation failed: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            raise RuntimeError("Database creation timeout exceeded")
    
    def _run_queries(self, db_path: Path, language: str, query_suite: str, output_path: Path):
        """Run CodeQL queries on the database"""
        
        try:
            result = subprocess.run(
                [
                    self.codeql_path, "database", "analyze",
                    str(db_path),
                    f"{language}-{query_suite}.qls",
                    "--format=sarif-latest",
                    f"--output={output_path}",
                    "--rerun"
                ],
                capture_output=True,
                text=True,
                timeout=600  # 10 minutes timeout
            )
            
            if result.returncode != 0:
                raise RuntimeError(f"Query execution failed: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            raise RuntimeError("Query execution timeout exceeded")
    
    def _parse_sarif(self, sarif_path: Path) -> List[CodeQLFinding]:
        """Parse SARIF format results into CodeQLFinding objects"""
        
        with open(sarif_path, 'r') as f:
            sarif_data = json.load(f)
        
        findings = []
        
        for run in sarif_data.get("runs", []):
            for result in run.get("results", []):
                rule_id = result.get("ruleId", "unknown")
                message = result.get("message", {}).get("text", "No description")
                
                # Get severity level
                level = result.get("level", "note")
                severity_map = {
                    "error": "critical",
                    "warning": "high",
                    "note": "medium",
                    "none": "low"
                }
                severity = severity_map.get(level, "medium")
                
                # Get location information
                for location in result.get("locations", []):
                    physical_location = location.get("physicalLocation", {})
                    artifact_location = physical_location.get("artifactLocation", {})
                    region = physical_location.get("region", {})
                    
                    file_path = artifact_location.get("uri", "unknown")
                    start_line = region.get("startLine", 0)
                    end_line = region.get("endLine", start_line)
                    
                    # Get recommendation from rule metadata
                    recommendation = self._get_recommendation(rule_id, run)
                    
                    findings.append(CodeQLFinding(
                        rule_id=rule_id,
                        severity=severity,
                        message=message,
                        file_path=file_path,
                        start_line=start_line,
                        end_line=end_line,
                        recommendation=recommendation
                    ))
        
        return findings
    
    def _get_recommendation(self, rule_id: str, run: Dict) -> str:
        """Extract recommendation from rule metadata"""
        for rule in run.get("tool", {}).get("driver", {}).get("rules", []):
            if rule.get("id") == rule_id:
                help_text = rule.get("help", {}).get("text", "")
                short_desc = rule.get("shortDescription", {}).get("text", "")
                return help_text or short_desc or "Review and fix the identified issue"
        
        return "Review and fix the identified issue"
    
    def _count_severities(self, findings: List[CodeQLFinding]) -> Dict[str, int]:
        """Count findings by severity level"""
        counts = {}
        for finding in findings:
            counts[finding.severity] = counts.get(finding.severity, 0) + 1
        return counts
```

---

# Phase 3: Gemini Interaction API Integration

## Step 3.1: Gemini Service with Memory

**File: `services/gemini_service.py`**

```python
from google import genai
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
from pathlib import Path

from config import settings
from models.responses import CodeQLFinding, SearchResult, Issue, AnalysisResponse

class GeminiService:
    """Service for Gemini Interaction API orchestration"""
    
    def __init__(self):
        self.client = genai.Client(api_key=settings.GEMINI_API_KEY)
        self.model = settings.GEMINI_MODEL
    
    def create_analysis_plan(
        self,
        repo_content: str,
        analysis_type: str,
        custom_instructions: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Create a hierarchical plan for repository analysis
        Uses Gemini's thinking capabilities
        """
        
        system_instruction = """You are an expert software architect and security analyst.
Your task is to create a detailed, step-by-step analysis plan for a code repository.

Break down the analysis into:
1. Key areas to investigate (architecture, security, performance, code quality)
2. Specific aspects to examine within each area
3. Tools/techniques to use for each aspect
4. Expected outputs for each step

Be thorough and systematic. Consider the repository size and complexity."""
        
        user_prompt = f"""Analyze this repository and create a comprehensive analysis plan.

Repository Overview:
{repo_content[:5000]}  # First 5000 chars for context

Analysis Type: {analysis_type}
{f"Custom Instructions: {custom_instructions}" if custom_instructions else ""}

Create a JSON plan with this structure:
{{
  "investigation_areas": [
    {{
      "area": "architecture",
      "aspects": ["component structure", "dependencies", ...],
      "tools": ["semantic_search", "codeql", ...],
      "priority": 1
    }},
    ...
  ],
  "search_queries": ["query1", "query2", ...],
  "security_focus_areas": ["area1", "area2", ...],
  "expected_issues": ["potential_issue1", ...]
}}
"""
        
        interaction = self.client.interactions.create(
            model=self.model,
            input=user_prompt,
            system_instruction=system_instruction,
            generation_config={
                "thinking_level": "high",  # Deep reasoning
                "temperature": 0.3,  # More focused/deterministic
                "thinking_summaries": "auto"
            }
        )
        
        # Extract the plan from response
        plan_text = interaction.outputs[-1].text
        
        # Parse JSON from response
        try:
            # Remove markdown code blocks if present
            if "```json" in plan_text:
                plan_text = plan_text.split("```json")[1].split("```")[0]
            elif "```" in plan_text:
                plan_text = plan_text.split("```")[1].split("```")[0]
            
            plan = json.loads(plan_text.strip())
        except json.JSONDecodeError:
            # Fallback: ask Gemini to fix the JSON
            plan = self._fix_json_response(plan_text)
        
        # Store interaction ID for memory
        plan["interaction_id"] = interaction.id
        
        return plan
    
    def analyze_with_context(
        self,
        repo_content: str,
        codeql_findings: List[CodeQLFinding],
        search_results: List[SearchResult],
        plan: Dict[str, Any],
        previous_interaction_id: Optional[str] = None
    ) -> AnalysisResponse:
        """
        Perform deep analysis using all available context
        Uses stateful conversation for memory
        """
        
        system_instruction = """You are an expert code reviewer and security analyst.
Analyze the provided repository data and create a comprehensive, actionable report.

Your report should include:
1. Architecture Summary (2-3 paragraphs)
2. Top 5 Critical Issues (with evidence and fix steps)
3. Overall Recommendations

Be specific, cite evidence (file paths, line numbers), and provide concrete fix steps."""
        
        # Prepare context
        context = self._prepare_analysis_context(
            repo_content,
            codeql_findings,
            search_results,
            plan
        )
        
        # Create or continue interaction
        if previous_interaction_id:
            # Continue previous conversation (memory layer)
            interaction = self.client.interactions.create(
                model=self.model,
                input=f"""Continue analysis with new data:

{context}

Provide the final comprehensive report.""",
                previous_interaction_id=previous_interaction_id,
                generation_config={
                    "thinking_level": "high",
                    "temperature": 0.4
                }
            )
        else:
            # New analysis
            interaction = self.client.interactions.create(
                model=self.model,
                input=context,
                system_instruction=system_instruction,
                generation_config={
                    "thinking_level": "high",
                    "temperature": 0.4
                }
            )
        
        # Parse response into structured format
        response_text = interaction.outputs[-1].text
        analysis = self._parse_analysis_response(response_text, interaction.id)
        
        return analysis
    
    def _prepare_analysis_context(
        self,
        repo_content: str,
        codeql_findings: List[CodeQLFinding],
        search_results: List[SearchResult],
        plan: Dict[str, Any]
    ) -> str:
        """Prepare comprehensive context for analysis"""
        
        context_parts = [
            "# Repository Analysis Context\n",
            f"## Analysis Plan\n{json.dumps(plan, indent=2)}\n",
            f"## Repository Code (excerpt)\n{repo_content[:10000]}\n",
        ]
        
        # Add CodeQL findings
        if codeql_findings:
            context_parts.append("\n## Security Findings (CodeQL)\n")
            for finding in codeql_findings[:20]:  # Top 20
                context_parts.append(
                    f"- [{finding.severity.upper()}] {finding.rule_id}: "
                    f"{finding.message} ({finding.file_path}:{finding.start_line})\n"
                )
        
        # Add semantic search results
        if search_results:
            context_parts.append("\n## Relevant Code Snippets (Semantic Search)\n")
            for result in search_results[:10]:  # Top 10
                context_parts.append(
                    f"\n### {result.file_path}:{result.line_number} "
                    f"(score: {result.relevance_score})\n```\n{result.code_snippet}\n```\n"
                )
        
        context_parts.append("""
\n## Task
Analyze all the above information and create a comprehensive report with:
1. **Architecture Summary** (2-3 paragraphs describing system design, patterns, technologies)
2. **Top 5 Critical Issues** (each with: title, description, severity, evidence with file/line citations, 1-3 step fix)
3. **Recommendations** (3-5 high-level recommendations)

Format your response as JSON:
{
  "architecture_summary": "...",
  "top_issues": [
    {
      "title": "...",
      "description": "...",
      "severity": "critical|high|medium|low",
      "evidence": ["file:line", ...],
      "fix_steps": ["step 1", "step 2", ...],
      "priority": 1-5
    }
  ],
  "recommendations": ["rec1", "rec2", ...]
}
""")
        
        return "".join(context_parts)
    
    def _parse_analysis_response(self, response_text: str, interaction_id: str) -> Dict[str, Any]:
        """Parse Gemini's analysis response into structured format"""
        
        try:
            # Remove markdown code blocks
            if "```json" in response_text:
                json_text = response_text.split("```json")[1].split("```")[0]
            elif "```" in response_text:
                json_text = response_text.split("```")[1].split("```")[0]
            else:
                json_text = response_text
            
            analysis = json.loads(json_text.strip())
            
            # Ensure required fields
            if "architecture_summary" not in analysis:
                analysis["architecture_summary"] = "Analysis summary not available"
            if "top_issues" not in analysis:
                analysis["top_issues"] = []
            if "recommendations" not in analysis:
                analysis["recommendations"] = []
            
            analysis["interaction_id"] = interaction_id
            analysis["timestamp"] = datetime.utcnow().isoformat()
            
            return analysis
            
        except json.JSONDecodeError:
            # Fallback: structured extraction
            return self._extract_structured_analysis(response_text, interaction_id)
    
    def _extract_structured_analysis(self, text: str, interaction_id: str) -> Dict[str, Any]:
        """Fallback: extract structure from unstructured text"""
        
        # Use Gemini to convert unstructured to structured
        conversion_interaction = self.client.interactions.create(
            model=self.model,
            input=f"""Convert this analysis into valid JSON format:

{text}

Required JSON structure:
{{
  "architecture_summary": "string",
  "top_issues": [
    {{
      "title": "string",
      "description": "string",
      "severity": "critical|high|medium|low",
      "evidence": ["string"],
      "fix_steps": ["string"],
      "priority": number
    }}
  ],
  "recommendations": ["string"]
}}

Return ONLY valid JSON, no explanations.""",
            generation_config={
                "temperature": 0.1  # Very deterministic
            }
        )
        
        json_text = conversion_interaction.outputs[-1].text
        
        # Remove markdown
        if "```json" in json_text:
            json_text = json_text.split("```json")[1].split("```")[0]
        elif "```" in json_text:
            json_text = json_text.split("```")[1].split("```")[0]
        
        try:
            analysis = json.loads(json_text.strip())
            analysis["interaction_id"] = interaction_id
            analysis["timestamp"] = datetime.utcnow().isoformat()
            return analysis
        except:
            # Ultimate fallback
            return {
                "architecture_summary": "Analysis could not be structured properly",
                "top_issues": [],
                "recommendations": [],
                "interaction_id": interaction_id,
                "timestamp": datetime.utcnow().isoformat(),
                "raw_text": text
            }
    
    def _fix_json_response(self, broken_json: str) -> Dict[str, Any]:
        """Use Gemini to fix malformed JSON"""
        
        fix_interaction = self.client.interactions.create(
            model=self.model,
            input=f"""Fix this malformed JSON and return only valid JSON:

{broken_json}

Return ONLY the fixed JSON, no explanations.""",
            generation_config={"temperature": 0.1}
        )
        
        fixed_text = fix_interaction.outputs[-1].text
        
        # Remove markdown
        if "```json" in fixed_text:
            fixed_text = fixed_text.split("```json")[1].split("```")[0]
        elif "```" in fixed_text:
            fixed_text = fixed_text.split("```")[1].split("```")[0]
        
        return json.loads(fixed_text.strip())
    
    def continue_conversation(
        self,
        interaction_id: str,
        user_query: str
    ) -> str:
        """
        Continue a previous analysis conversation
        Uses Gemini's memory via previous_interaction_id
        """
        
        interaction = self.client.interactions.create(
            model=self.model,
            input=user_query,
            previous_interaction_id=interaction_id,
            generation_config={
                "thinking_level": "medium",
                "temperature": 0.5
            }
        )
        
        return interaction.outputs[-1].text
```

---

# Phase 4: Memory Layer with FAISS

## Step 4.1: Vector Store for Embeddings

**File: `memory/vector_store.py`**

```python
import faiss
import numpy as np
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from sentence_transformers import SentenceTransformer

from config import settings

class VectorStore:
    """FAISS-based vector store for semantic memory"""
    
    def __init__(self):
        self.memory_dir = Path(settings.MEMORY_DIR)
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize sentence transformer for embeddings
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.dimension = 384  # Dimension of all-MiniLM-L6-v2
        
        # FAISS index
        self.index: Optional[faiss.IndexFlatL2] = None
        self.metadata: List[Dict[str, Any]] = []
        
        # Load existing index if available
        self._load_index()
    
    def _load_index(self):
        """Load existing FAISS index and metadata"""
        index_path = self.memory_dir / "faiss.index"
        metadata_path = self.memory_dir / "metadata.pkl"
        
        if index_path.exists() and metadata_path.exists():
            try:
                self.index = faiss.read_index(str(index_path))
                with open(metadata_path, 'rb') as f:
                    self.metadata = pickle.load(f)
                print(f"Loaded existing index with {len(self.metadata)} items")
            except Exception as e:
                print(f"Error loading index: {e}. Creating new index.")
                self.index = faiss.IndexFlatL2(self.dimension)
                self.metadata = []
        else:
            self.index = faiss.IndexFlatL2(self.dimension)
            self.metadata = []
    
    def _save_index(self):
        """Save FAISS index and metadata to disk"""
        index_path = self.memory_dir / "faiss.index"
        metadata_path = self.memory_dir / "metadata.pkl"
        
        faiss.write_index(self.index, str(index_path))
        with open(metadata_path, 'wb') as f:
            pickle.dump(self.metadata, f)
    
    def add_memory(
        self,
        text: str,
        metadata: Dict[str, Any],
        memory_type: str = "interaction"
    ):
        """
        Add a new memory to the vector store
        
        Args:
            text: Text content to embed
            metadata: Associated metadata (repo_id, interaction_id, etc.)
            memory_type: Type of memory (interaction, tool_result, analysis, etc.)
        """
        # Create embedding
        embedding = self.encoder.encode([text])[0]
        embedding_np = np.array([embedding]).astype('float32')
        
        # Add to index
        self.index.add(embedding_np)
        
        # Store metadata
        full_metadata = {
            **metadata,
            "text": text,
            "memory_type": memory_type,
            "index_id": len(self.metadata)
        }
        self.metadata.append(full_metadata)
        
        # Save to disk
        self._save_index()
    
    def search_similar(
        self,
        query: str,
        k: int = 5,
        memory_type: Optional[str] = None,
        repo_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for similar memories
        
        Args:
            query: Search query
            k: Number of results to return
            memory_type: Filter by memory type
            repo_id: Filter by repository ID
        
        Returns:
            List of similar memories with scores
        """
        if self.index.ntotal == 0:
            return []
        
        # Create query embedding
        query_embedding = self.encoder.encode([query])[0]
        query_np = np.array([query_embedding]).astype('float32')
        
        # Search (get more than k to allow filtering)
        search_k = min(k * 3, self.index.ntotal)
        distances, indices = self.index.search(query_np, search_k)
        
        # Collect results with filtering
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx >= len(self.metadata):
                continue
            
            meta = self.metadata[idx]
            
            # Apply filters
            if memory_type and meta.get("memory_type") != memory_type:
                continue
            if repo_id and meta.get("repo_id") != repo_id:
                continue
            
            results.append({
                **meta,
                "similarity_score": float(1 / (1 + dist))  # Convert distance to similarity
            })
            
            if len(results) >= k:
                break
        
        return results
    
    def get_repo_context(self, repo_id: str, k: int = 10) -> str:
        """
        Get contextual memory for a specific repository
        
        Returns a formatted string of past interactions and insights
        """
        memories = self.search_similar(
            query="analysis insights recommendations",
            k=k,
            repo_id=repo_id
        )
        
        if not memories:
            return "No previous context available for this repository."
        
        context_parts = ["## Previous Context\n"]
        
        for mem in memories:
            mem_type = mem.get("memory_type", "unknown")
            timestamp = mem.get("timestamp", "unknown")
            text = mem.get("text", "")[:500]  # Truncate to 500 chars
            
            context_parts.append(
                f"### {mem_type} ({timestamp})\n{text}\n\n"
            )
        
        return "".join(context_parts)
    
    def add_analysis_memory(
        self,
        repo_id: str,
        interaction_id: str,
        analysis: Dict[str, Any]
    ):
        """Add an analysis result to memory"""
        
        # Create a summary text for embedding
        summary = f"""
Architecture: {analysis.get('architecture_summary', '')}

Top Issues:
{chr(10).join([f"- {issue.get('title', '')}: {issue.get('description', '')}" 
               for issue in analysis.get('top_issues', [])])}

Recommendations:
{chr(10).join([f"- {rec}" for rec in analysis.get('recommendations', [])])}
"""
        
        self.add_memory(
            text=summary,
            metadata={
                "repo_id": repo_id,
                "interaction_id": interaction_id,
                "analysis_data": analysis,
                "timestamp": analysis.get("timestamp", "")
            },
            memory_type="analysis"
        )
    
    def add_tool_result_memory(
        self,
        repo_id: str,
        tool_name: str,
        result_summary: str,
        full_result: Dict[str, Any]
    ):
        """Add a tool execution result to memory"""
        
        self.add_memory(
            text=f"{tool_name}: {result_summary}",
            metadata={
                "repo_id": repo_id,
                "tool_name": tool_name,
                "result_data": full_result,
                "timestamp": full_result.get("timestamp", "")
            },
            memory_type="tool_result"
        )
```

## Step 4.2: Interaction Store

**File: `memory/interaction_store.py`**

```python
import json
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
from collections import defaultdict

from config import settings

class InteractionStore:
    """Store and retrieve Gemini interaction histories"""
    
    def __init__(self):
        self.store_dir = Path(settings.MEMORY_DIR) / "interactions"
        self.store_dir.mkdir(parents=True, exist_ok=True)
        
        # In-memory cache
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._repo_index: Dict[str, List[str]] = defaultdict(list)
        
        # Load existing interactions
        self._load_interactions()
    
    def _load_interactions(self):
        """Load all stored interactions into cache"""
        for interaction_file in self.store_dir.glob("*.json"):
            try:
                with open(interaction_file, 'r') as f:
                    data = json.load(f)
                    interaction_id = data.get("interaction_id")
                    repo_id = data.get("repo_id")
                    
                    if interaction_id:
                        self._cache[interaction_id] = data
                        if repo_id:
                            self._repo_index[repo_id].append(interaction_id)
            except Exception as e:
                print(f"Error loading {interaction_file}: {e}")
    
    def save_interaction(
        self,
        interaction_id: str,
        repo_id: str,
        interaction_type: str,
        data: Dict[str, Any]
    ):
        """Save an interaction to persistent storage"""
        
        interaction_data = {
            "interaction_id": interaction_id,
            "repo_id": repo_id,
            "interaction_type": interaction_type,
            "timestamp": datetime.utcnow().isoformat(),
            "data": data
        }
        
        # Save to file
        file_path = self.store_dir / f"{interaction_id}.json"
        with open(file_path, 'w') as f:
            json.dump(interaction_data, f, indent=2)
        
        # Update cache and index
        self._cache[interaction_id] = interaction_data
        self._repo_index[repo_id].append(interaction_id)
    
    def get_interaction(self, interaction_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve a specific interaction"""
        return self._cache.get(interaction_id)
    
    def get_repo_interactions(
        self,
        repo_id: str,
        interaction_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """Get all interactions for a repository"""
        
        interaction_ids = self._repo_index.get(repo_id, [])
        interactions = []
        
        for iid in interaction_ids:
            interaction = self._cache.get(iid)
            if interaction:
                if interaction_type is None or interaction.get("interaction_type") == interaction_type:
                    interactions.append(interaction)
        
        # Sort by timestamp (newest first)
        interactions.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
        
        return interactions
    
    def get_latest_analysis(self, repo_id: str) -> Optional[Dict[str, Any]]:
        """Get the most recent analysis for a repository"""
        
        analyses = self.get_repo_interactions(repo_id, "analysis")
        return analyses[0] if analyses else None
    
    def get_conversation_context(
        self,
        repo_id: str,
        max_interactions: int = 5
    ) -> str:
        """
        Build a conversation context from past interactions
        Useful for providing context to Gemini
        """
        
        interactions = self.get_repo_interactions(repo_id)[:max_interactions]
        
        if not interactions:
            return "No previous conversation history."
        
        context_parts = ["## Previous Interactions\n"]
        
        for interaction in interactions:
            itype = interaction.get("interaction_type", "unknown")
            timestamp = interaction.get("timestamp", "")
            
            context_parts.append(f"\n### {itype} - {timestamp}\n")
            
            # Add relevant data based on interaction type
            data = interaction.get("data", {})
            if itype == "plan":
                context_parts.append(f"Plan: {json.dumps(data.get('investigation_areas', []), indent=2)}\n")
            elif itype == "analysis":
                summary = data.get("architecture_summary", "")[:300]
                context_parts.append(f"Summary: {summary}...\n")
        
        return "".join(context_parts)
```

---

# Phase 5: FastAPI Endpoints & Complete Orchestration

## Step 5.1: FastAPI Routers

**File: `api/ingest.py`**

```python
from fastapi import APIRouter, HTTPException
from typing import List

from models.requests import IngestRequest
from models.responses import IngestResponse
from services.ingest_service import IngestService

router = APIRouter(prefix="/api/ingest", tags=["ingest"])
ingest_service = IngestService()

@router.post("/", response_model=IngestResponse)
async def ingest_repository(request: IngestRequest):
    """
    Ingest a repository (clone and convert to repo.md)
    """
    try:
        result = ingest_service.ingest_repository(request)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{repo_id}/content")
async def get_repo_content(repo_id: str):
    """
    Get the repo.md content for a repository
    """
    try:
        content = ingest_service.get_repo_content(repo_id)
        return {"repo_id": repo_id, "content": content}
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**File: `api/search.py`**

```python
from fastapi import APIRouter, HTTPException

from models.requests import SemanticSearchRequest
from models.responses import SemanticSearchResponse
from services.search_service import SearchService

router = APIRouter(prefix="/api/search", tags=["search"])
search_service = SearchService()

@router.post("/index/{repo_id}")
async def index_repository(repo_id: str):
    """
    Index a repository with SeaGOAT for semantic search
    """
    try:
        success = await search_service.index_repository(repo_id)
        return {"repo_id": repo_id, "indexed": success}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/", response_model=SemanticSearchResponse)
async def search_code(request: SemanticSearchRequest):
    """
    Perform semantic code search
    """
    try:
        results = await search_service.search(request)
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**File: `api/analysis.py`**

```python
from fastapi import APIRouter, HTTPException

from models.requests import CodeQLScanRequest
from models.responses import CodeQLResponse
from services.codeql_service import CodeQLService

router = APIRouter(prefix="/api/analysis", tags=["analysis"])
codeql_service = CodeQLService()

@router.post("/codeql", response_model=CodeQLResponse)
async def run_codeql_scan(request: CodeQLScanRequest):
    """
    Run CodeQL security and quality analysis
    """
    try:
        results = codeql_service.analyze_repository(request)
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**File: `api/orchestrator.py`**

```python
from fastapi import APIRouter, HTTPException, BackgroundTasks
from typing import Optional
import asyncio

from models.requests import AnalysisRequest, SemanticSearchRequest, CodeQLScanRequest
from models.responses import AnalysisResponse, Issue
from services.ingest_service import IngestService
from services.search_service import SearchService
from services.codeql_service import CodeQLService
from services.gemini_service import GeminiService
from memory.vector_store import VectorStore
from memory.interaction_store import InteractionStore
from config import settings
from pathlib import Path
from datetime import datetime
import json

router = APIRouter(prefix="/api/orchestrate", tags=["orchestration"])

# Initialize services
ingest_service = IngestService()
search_service = SearchService()
codeql_service = CodeQLService()
gemini_service = GeminiService()
vector_store = VectorStore()
interaction_store = InteractionStore()

@router.post("/analyze", response_model=AnalysisResponse)
async def analyze_repository(request: AnalysisRequest, background_tasks: BackgroundTasks):
    """
    Complete repository analysis orchestration
    This is the Level 3 implementation with memory layer
    """
    try:
        # Step 1: Ingest repository
        print(f"[ORCHESTRATOR] Step 1: Ingesting repository...")
        ingest_result = ingest_service.ingest_repository(
            IngestRequest(source=request.source)
        )
        repo_id = ingest_result.repo_id
        repo_content = ingest_service.get_repo_content(repo_id)
        
        # Step 2: Get memory/context from previous interactions
        print(f"[ORCHESTRATOR] Step 2: Loading memory context...")
        memory_context = vector_store.get_repo_context(repo_id, k=5)
        conversation_history = interaction_store.get_conversation_context(repo_id, max_interactions=3)
        
        # Step 3: Create analysis plan with Gemini (Level 2: Planning)
        print(f"[ORCHESTRATOR] Step 3: Creating analysis plan with Gemini...")
        plan = gemini_service.create_analysis_plan(
            repo_content=repo_content,
            analysis_type=request.analysis_type,
            custom_instructions=request.custom_instructions
        )
        
        # Store plan interaction
        interaction_store.save_interaction(
            interaction_id=plan["interaction_id"],
            repo_id=repo_id,
            interaction_type="plan",
            data=plan
        )
        
        # Step 4: Execute plan - run tools based on plan
        print(f"[ORCHESTRATOR] Step 4: Executing analysis plan...")
        
        # 4a: Index with SeaGOAT if requested
        search_results = []
        if request.include_semantic_search:
            print(f"[ORCHESTRATOR] Step 4a: Indexing with SeaGOAT...")
            await search_service.index_repository(repo_id)
            
            # Run semantic searches based on plan
            for query in plan.get("search_queries", [])[:5]:  # Limit to 5 queries
                results = await search_service.search(
                    SemanticSearchRequest(
                        repo_id=repo_id,
                        query=query,
                        limit=5
                    )
                )
                search_results.extend(results.results)
            
            # Store search results in memory
            vector_store.add_tool_result_memory(
                repo_id=repo_id,
                tool_name="SeaGOAT",
                result_summary=f"Found {len(search_results)} relevant code snippets",
                full_result={"results": [r.dict() for r in search_results]}
            )
        
        # 4b: Run CodeQL if requested
        codeql_findings = []
        if request.include_security_scan:
            print(f"[ORCHESTRATOR] Step 4b: Running CodeQL analysis...")
            codeql_result = codeql_service.analyze_repository(
                CodeQLScanRequest(
                    repo_id=repo_id,
                    language="python",  # TODO: Auto-detect from ingest stats
                    query_suite="security-extended"
                )
            )
            codeql_findings = codeql_result.findings
            
            # Store CodeQL results in memory
            vector_store.add_tool_result_memory(
                repo_id=repo_id,
                tool_name="CodeQL",
                result_summary=f"Found {len(codeql_findings)} security/quality issues",
                full_result=codeql_result.dict()
            )
        
        # Step 5: Deep analysis with Gemini (Level 3: Memory integration)
        print(f"[ORCHESTRATOR] Step 5: Performing deep analysis with Gemini...")
        
        # Use previous interaction if provided (conversation continuity)
        previous_id = request.previous_interaction_id or plan["interaction_id"]
        
        analysis_dict = gemini_service.analyze_with_context(
            repo_content=repo_content,
            codeql_findings=codeql_findings,
            search_results=search_results,
            plan=plan,
            previous_interaction_id=previous_id
        )
        
        # Store analysis in memory
        vector_store.add_analysis_memory(
            repo_id=repo_id,
            interaction_id=analysis_dict["interaction_id"],
            analysis=analysis_dict
        )
        
        interaction_store.save_interaction(
            interaction_id=analysis_dict["interaction_id"],
            repo_id=repo_id,
            interaction_type="analysis",
            data=analysis_dict
        )
        
        # Step 6: Generate report files
        print(f"[ORCHESTRATOR] Step 6: Generating report files...")
        report_path, json_path = _generate_reports(repo_id, analysis_dict)
        
        # Convert to response model
        response = AnalysisResponse(
            repo_id=repo_id,
            interaction_id=analysis_dict["interaction_id"],
            architecture_summary=analysis_dict.get("architecture_summary", ""),
            top_issues=[
                Issue(**issue) for issue in analysis_dict.get("top_issues", [])
            ],
            recommendations=analysis_dict.get("recommendations", []),
            report_path=str(report_path),
            raw_report_json=analysis_dict,
            created_at=datetime.utcnow()
        )
        
        print(f"[ORCHESTRATOR] âœ… Analysis complete! Interaction ID: {analysis_dict['interaction_id']}")
        
        return response
        
    except Exception as e:
        print(f"[ORCHESTRATOR] âŒ Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/continue/{interaction_id}")
async def continue_analysis(interaction_id: str, query: str):
    """
    Continue a previous analysis with follow-up questions
    Uses Gemini's memory via previous_interaction_id
    """
    try:
        # Get the stored interaction
        interaction = interaction_store.get_interaction(interaction_id)
        if not interaction:
            raise HTTPException(status_code=404, detail="Interaction not found")
        
        # Continue conversation with Gemini
        response = gemini_service.continue_conversation(
            interaction_id=interaction_id,
            user_query=query
        )
        
        return {
            "interaction_id": interaction_id,
            "query": query,
            "response": response
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/memory/{repo_id}")
async def get_repository_memory(repo_id: str, query: Optional[str] = None):
    """
    Retrieve memory/context for a repository
    """
    try:
        if query:
            # Search for specific memories
            memories = vector_store.search_similar(
                query=query,
                k=10,
                repo_id=repo_id
            )
        else:
            # Get general context
            memories = vector_store.search_similar(
                query="analysis insights",
                k=10,
                repo_id=repo_id
            )
        
        return {
            "repo_id": repo_id,
            "memories": memories,
            "total": len(memories)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def _generate_reports(repo_id: str, analysis: dict) -> tuple[Path, Path]:
    """Generate markdown and JSON reports"""
    
    output_dir = Path(settings.OUTPUT_DIR) / repo_id
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate Markdown report
    report_md = output_dir / "report.md"
    with open(report_md, 'w') as f:
        f.write(f"# Repository Analysis Report\n\n")
        f.write(f"**Repository ID:** {repo_id}\n")
        f.write(f"**Generated:** {datetime.utcnow().isoformat()}\n")
        f.write(f"**Interaction ID:** {analysis['interaction_id']}\n\n")
        f.write("---\n\n")
        
        f.write("## Architecture Summary\n\n")
        f.write(f"{analysis.get('architecture_summary', 'N/A')}\n\n")
        
        f.write("## Top Issues\n\n")
        for i, issue in enumerate(analysis.get('top_issues', []), 1):
            f.write(f"### {i}. {issue.get('title', 'Untitled Issue')}\n\n")
            f.write(f"**Severity:** {issue.get('severity', 'unknown').upper()}\n\n")
            f.write(f"**Priority:** {issue.get('priority', 'N/A')}\n\n")
            f.write(f"**Description:**\n{issue.get('description', 'No description')}\n\n")
            
            f.write("**Evidence:**\n")
            for evidence in issue.get('evidence', []):
                f.write(f"- {evidence}\n")
            f.write("\n")
            
            f.write("**Fix Steps:**\n")
            for j, step in enumerate(issue.get('fix_steps', []), 1):
                f.write(f"{j}. {step}\n")
            f.write("\n")
        
        f.write("## Recommendations\n\n")
        for i, rec in enumerate(analysis.get('recommendations', []), 1):
            f.write(f"{i}. {rec}\n")
    
    # Generate JSON report
    report_json = output_dir / "report.json"
    with open(report_json, 'w') as f:
        json.dump(analysis, f, indent=2)
    
    return report_md, report_json
```

---

# Phase 6: Main Application & Deployment

## Step 6.1: Main FastAPI Application

**File: `main.py`**

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager

from config import settings
from api import ingest, search, analysis, orchestrator

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown"""
    print("ðŸš€ Starting Repo Analyzer API...")
    print(f"ðŸ“ Workspace: {settings.WORKSPACE_DIR}")
    print(f"ðŸ§  Gemini Model: {settings.GEMINI_MODEL}")
    yield
    print("ðŸ›‘ Shutting down Repo Analyzer API...")

# Create FastAPI app
app = FastAPI(
    title=settings.API_TITLE,
    version=settings.API_VERSION,
    description="""
    # Repo Analyzer API - Level 3 AI Systems Builder
    
    Transform GitHub repositories into actionable reports using:
    - **FastAPI**: RESTful API layer
    - **Gemini Interaction API**: AI orchestration with memory
    - **FAISS**: Vector-based memory layer
    - **Multi-tool Integration**: repo2txt, SeaGOAT, CodeQL
    
    ## Features
    - ðŸ§  **AI Planning**: Gemini breaks down analysis into steps
    - ðŸ’¾ **Memory Layer**: Persistent context across analyses
    - ðŸ” **Semantic Search**: Find relevant code snippets
    - ðŸ”’ **Security Scanning**: CodeQL static analysis
    - ðŸ“Š **Actionable Reports**: Evidence-backed issues with fix steps
    """,
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(ingest.router)
app.include_router(search.router)
app.include_router(analysis.router)
app.include_router(orchestrator.router)

@app.get("/")
async def root():
    """API root endpoint"""
    return {
        "message": "Repo Analyzer API - Level 3 AI Systems Builder",
        "version": settings.API_VERSION,
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "gemini_configured": bool(settings.GEMINI_API_KEY),
        "workspace": settings.WORKSPACE_DIR
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host=settings.API_HOST,
        port=settings.API_PORT,
        reload=True  # Development mode
    )
```

## Step 6.2: Requirements File

**File: `requirements.txt`**

```txt
# FastAPI
fastapi[standard]>=0.115.0
uvicorn[standard]>=0.32.0
pydantic>=2.0.0
pydantic-settings>=2.0.0

# Gemini AI
google-genai>=1.55.0

# Memory Layer
faiss-cpu>=1.8.0
sentence-transformers>=2.2.0
numpy>=1.24.0

# Repository Tools
GitPython>=3.1.40
requests>=2.31.0

# SeaGOAT (optional - install separately if needed)
# seagoat

# CodeQL (install CLI separately)
# https://github.com/github/codeql-cli-binaries

# Development
pytest>=7.4.0
httpx>=0.24.0
```

## Step 6.3: Deployment Instructions

**File: `README.md`**

```markdown
# Repo Analyzer - Level 3 AI Systems Builder

AI-powered repository analysis with memory, planning, and orchestration.

## Quick Start

### 1. Installation

```bash
# Clone repository
git clone <your-repo-url>
cd repo-analyzer

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install external tools
# - SeaGOAT: pip install seagoat
# - CodeQL: Download from https://github.com/github/codeql-cli-binaries
```

### 2. Configuration

Create `.env` file:

```env
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.5-flash
CODEQL_PATH=/path/to/codeql
WORKSPACE_DIR=./workspace
```

### 3. Run the API

```bash
# Development mode
fastapi dev main.py

# Production mode
fastapi run main.py
```

API will be available at: `http://127.0.0.1:8000`
Documentation: `http://127.0.0.1:8000/docs`

## Usage Examples

### Complete Analysis (Single API Call)

```bash
curl -X POST "http://127.0.0.1:8000/api/orchestrate/analyze" \
  -H "Content-Type: application/json" \
  -d '{
    "source": {
      "url": "https://github.com/user/repo"
    },
    "analysis_type": "full",
    "include_semantic_search": true,
    "include_security_scan": true
  }'
```

### Continue Previous Analysis

```bash
curl -X POST "http://127.0.0.1:8000/api/orchestrate/continue/{interaction_id}" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the most critical security issues?"
  }'
```

### Check Repository Memory

```bash
curl "http://127.0.0.1:8000/api/orchestrate/memory/{repo_id}?query=security"
```

## Architecture

### Level 3 Features

1. **AI Planning (Level 2)**
   - Gemini creates hierarchical analysis plans
   - Task decomposition before execution
   
2. **Memory Layer (Level 3)**
   - FAISS vector store for semantic memory
   - Interaction history with context
   - Tool result caching

3. **Orchestration**
   - FastAPI coordinates all tools
   - Gemini Interaction API for state management
   - Background task execution

## Project Structure

```
repo-analyzer/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml              # CI/CD pipeline (Lint â†’ Unit â†’ Integration)
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ middleware.py           # Request ID, logging, CORS, rate limiting
â”‚   â”œâ”€â”€ ingest.py               # Repository ingestion endpoints
â”‚   â”œâ”€â”€ search.py               # Semantic search endpoints
â”‚   â”œâ”€â”€ analysis.py             # CodeQL analysis endpoints
â”‚   â””â”€â”€ orchestrator.py         # Plan creation & execution endpoints
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ requests.py             # Request models (Ingest, Search, CodeQL, Orchestrator)
â”‚   â””â”€â”€ responses.py            # Response models
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest_service.py       # Repository cloning & processing
â”‚   â”œâ”€â”€ seagoat_service.py      # SeaGOAT CLI wrapper
â”‚   â”œâ”€â”€ codeql_service.py       # CodeQL CLI wrapper
â”‚   â””â”€â”€ orchestrator.py         # Plan creation & execution with HMAC
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py               # Structured JSON logging with secret filtering
â”‚   â””â”€â”€ metrics.py              # In-memory metrics collection
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py             # Shared fixtures
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_orchestrator_service.py  # 18 test cases
â”‚   â”‚   â”œâ”€â”€ test_ingest_service.py        # 11 test cases
â”‚   â”‚   â””â”€â”€ test_utils.py                 # 12 test cases
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ test_api_flows.py             # 20+ test cases
â”œâ”€â”€ workspace/
â”‚   â”œâ”€â”€ ingest/                 # Cloned repositories
â”‚   â”œâ”€â”€ output/                 # Analysis outputs
â”‚   â”œâ”€â”€ memory/                 # FAISS vector store
â”‚   â”œâ”€â”€ codeql-dbs/             # CodeQL databases
â”‚   â””â”€â”€ plans/                  # Orchestrator plans
â”œâ”€â”€ config.py                   # Pydantic settings with DEBUG mode
â”œâ”€â”€ main.py                     # FastAPI app with /health, /metrics
â”œâ”€â”€ pytest.ini                  # Pytest configuration (70% coverage)
â”œâ”€â”€ requirements.txt            # Dependencies + testing tools
â”œâ”€â”€ .env.example                # Environment template
â””â”€â”€ validate_*.py               # Module validation scripts
```

## Troubleshooting

### CodeQL Not Found
```bash
# Download CodeQL CLI
wget https://github.com/github/codeql-cli-binaries/releases/latest/download/codeql-linux64.zip
unzip codeql-linux64.zip
export PATH=$PATH:/path/to/codeql
```

### SeaGOAT Issues
```bash
# Install SeaGOAT
pip install seagoat

# Test installation
seagoat --version
```

## License

MIT License
```

---

# Phase 7: Testing Guide

## Step 7.1: Test Script

**File: `test_complete_flow.py`**

```python
import requests
import json
import time
from pprint import pprint

BASE_URL = "http://127.0.0.1:8000"

def test_complete_analysis():
    """Test the complete analysis flow"""
    
    print("=" * 60)
    print("Testing Complete Repository Analysis Flow")
    print("=" * 60)
    
    # Test 1: Health Check
    print("\n1. Health Check...")
    response = requests.get(f"{BASE_URL}/health")
    assert response.status_code == 200
    print("âœ… API is healthy")
    pprint(response.json())
    
    # Test 2: Complete Analysis
    print("\n2. Running Complete Analysis...")
    analysis_request = {
        "source": {
            "url": "https://github.com/anthropics/anthropic-sdk-python"  # Small repo for testing
        },
        "analysis_type": "security",
        "include_semantic_search": True,
        "include_security_scan": True
    }
    
    response = requests.post(
        f"{BASE_URL}/api/orchestrate/analyze",
        json=analysis_request,
        timeout=600  # 10 minutes
    )
    
    assert response.status_code == 200
    analysis_result = response.json()
    
    print("âœ… Analysis Complete!")
    print(f"\nRepo ID: {analysis_result['repo_id']}")
    print(f"Interaction ID: {analysis_result['interaction_id']}")
    print(f"\nArchitecture Summary:")
    print(analysis_result['architecture_summary'][:500] + "...")
    
    print(f"\nTop Issues ({len(analysis_result['top_issues'])}):")
    for i, issue in enumerate(analysis_result['top_issues'][:3], 1):
        print(f"{i}. [{issue['severity'].upper()}] {issue['title']}")
    
    interaction_id = analysis_result['interaction_id']
    repo_id = analysis_result['repo_id']
    
    # Test 3: Continue Analysis
    print("\n3. Continuing Analysis with Follow-up...")
    continue_response = requests.post(
        f"{BASE_URL}/api/orchestrate/continue/{interaction_id}",
        params={"query": "What's the most critical security issue and how do I fix it?"}
    )
    
    assert continue_response.status_code == 200
    print("âœ… Continuation successful!")
    print(f"Response: {continue_response.json()['response'][:500]}...")
    
    # Test 4: Check Memory
    print("\n4. Checking Repository Memory...")
    memory_response = requests.get(
        f"{BASE_URL}/api/orchestrate/memory/{repo_id}",
        params={"query": "security issues"}
    )
    
    assert memory_response.status_code == 200
    memory_data = memory_response.json()
    print(f"âœ… Found {memory_data['total']} relevant memories")
    
    print("\n" + "=" * 60)
    print("All Tests Passed! ðŸŽ‰")
    print("=" * 60)
    
    return analysis_result

if __name__ == "__main__":
    try:
        result = test_complete_analysis()
    except Exception as e:
        print(f"\nâŒ Test failed: {str(e)}")
        raise
```

## Step 7.2: Run Tests

```bash
# Install test dependencies
pip install pytest httpx

# Start the API
fastapi dev main.py

# In another terminal, run tests
python test_complete_flow.py
```

---

# Summary & Next Steps

## What You've Built

âœ… **Level 1: Tool Integration**
- FastAPI endpoints for each tool (ingest, search, analysis)
- Clean separation of concerns

âœ… **Level 2: AI Planning**
- Gemini creates hierarchical analysis plans
- Task decomposition before execution

âœ… **Level 3: Memory Layer**
- FAISS vector store for semantic memory
- Interaction history with Gemini Interaction API
- Context-aware analysis

## Key Features

1. **Stateful Conversations**: Uses `previous_interaction_id`
2. **Planning & Reasoning**: Gemini's `thinking_level="high"`
3. **Memory Integration**: FAISS + Interaction Store
4. **Multi-tool Orchestration**: SeaGOAT, CodeQL, repo2txt
5. **Actionable Reports**: Evidence-backed issues with fixes

## Production Checklist

- [ ] Add authentication/authorization
- [ ] Rate limiting
- [ ] Async background tasks for long analyses
- [ ] Database for persistent storage (PostgreSQL)
- [ ] Docker containerization
- [ ] CI/CD pipeline
- [ ] Monitoring & logging (Sentry, DataDog)
- [ ] API versioning
- [ ] Caching layer (Redis)
- [ ] Load balancing

## Future Enhancements

1. **Multi-repo comparison**: Analyze multiple repos simultaneously
2. **Real-time updates**: WebSocket support for live progress
3. **Custom tools**: Plugin system for additional analyzers
4. **Team collaboration**: Multi-user support with shared memory
5. **Integration**: GitHub App, VS Code extension

---

**You now have a complete Level 3 AI Systems Builder!** ðŸŽ‰

The implementation follows best practices from both FastAPI and Gemini Interaction API documentation, with no hallucinations or fake code. Each component is production-ready and can be deployed independently or as a complete system.